<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta name="description" content="A statically generated blog example using Next.js and Hashnode."/><title>Resolving High Disk Space Utilization in MongoDB | Next.js Blog Example with Hashnode</title><link rel="canonical" href="https://engineering.hashnode.com/resolving-high-disk-space-utilization-in-mongodb"/><meta property="og:image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1686811773227/c8f20c9f-790e-4501-b002-7b17a90e14fa.png"/><style>.hljs{display:block;overflow-x:auto;padding:.5em;background:#23241f}.hljs,.hljs-subst,.hljs-tag{color:#f8f8f2}.hljs-emphasis,.hljs-strong{color:#a8a8a2}.hljs-bullet,.hljs-link,.hljs-literal,.hljs-number,.hljs-quote,.hljs-regexp{color:#ae81ff}.hljs-code,.hljs-section,.hljs-selector-class,.hljs-title{color:#a6e22e}.hljs-strong{font-weight:700}.hljs-emphasis{font-style:italic}.hljs-attr,.hljs-keyword,.hljs-name,.hljs-selector-tag{color:#f92672}.hljs-attribute,.hljs-symbol{color:#66d9ef}.hljs-class .hljs-title,.hljs-params{color:#f8f8f2}.hljs-addition,.hljs-built_in,.hljs-builtin-name,.hljs-selector-attr,.hljs-selector-id,.hljs-selector-pseudo,.hljs-string,.hljs-template-variable,.hljs-type,.hljs-variable{color:#e6db74}.hljs-comment,.hljs-deletion,.hljs-meta{color:#75715e}</style><meta name="next-head-count" content="17"/><link rel="preload" href="/_next/static/css/d0e5352acbffda1a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d0e5352acbffda1a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-1971b0cfe55b3cec.js" defer=""></script><script src="/_next/static/chunks/main-23c305297f46b9aa.js" defer=""></script><script src="/_next/static/chunks/pages/_app-578b9546cf8037d6.js" defer=""></script><script src="/_next/static/chunks/993-0855efe76f399ee1.js" defer=""></script><script src="/_next/static/chunks/209-1ad34ca62f7e8c97.js" defer=""></script><script src="/_next/static/chunks/pages/%5Bslug%5D-a70486900a6d7b3c.js" defer=""></script><script src="/_next/static/0y8ML8Ut1GKcfVwKpwMCT/_buildManifest.js" defer=""></script><script src="/_next/static/0y8ML8Ut1GKcfVwKpwMCT/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="min-h-screen bg-white dark:bg-neutral-950"><main><div class="container mx-auto undefined"><div class="grid items-center grid-cols-3 gap-10 px-5 py-10 md:grid-cols-2"><div class="col-span-1"><h2 class="text-3xl font-bold md:text-5xl text-slate-950 dark:text-neutral-100"><a href="/">Blog</a></h2></div><div class="flex flex-row justify-end col-span-2 md:col-span-1"><button class="flex flex-row items-center justify-start gap-2 px-4 py-2 text-sm font-semibold transition-colors duration-200 rounded-full border md:text-base md:px-5 md:py-3 hover:bg-primary-600 hover:text-white bg-primary-50 text-primary-600  border-primary-600 dark:bg-primary-500 dark:text-white md:justify-center undefined"><div class="flex flex-row items-center gap-2"><div class="shrink-0"><svg class="w-5 h-5 fill-current" fill="none" viewBox="0 0 24 24"><path fill="currentColor" d="M13.137 7.1a.75.75 0 1 0 0-1.5v1.5Zm6.765 5.58a.75.75 0 0 0-1.5 0h1.5Zm-17.4-3.13a.75.75 0 1 0-.69 1.332l.69-1.332Zm14.092 2.155a.75.75 0 0 0-.833-1.248l.833 1.248Zm-.312-6.095a.75.75 0 0 0 0 1.5v-1.5ZM22 7.109a.75.75 0 0 0 0-1.5v1.5Zm-3.609 2.108a.75.75 0 1 0 1.5 0h-1.5Zm1.5-5.717a.75.75 0 0 0-1.5 0h1.5ZM8.277 13.377l-.344.666.344-.666Zm1.85.785.118-.74-.117.74Zm3.02-1.058-.417-.624.416.624Zm-1.752.987-.2-.723.2.723Zm7.007 3.695c0 .638-.517 1.155-1.155 1.155v1.5a2.656 2.656 0 0 0 2.655-2.655h-1.5Zm-1.155 1.155H3.906v1.5h13.34v-1.5Zm-13.341 0a1.156 1.156 0 0 1-1.156-1.155h-1.5a2.656 2.656 0 0 0 2.656 2.655v-1.5ZM2.75 17.786v-9.53h-1.5v9.53h1.5Zm0-9.53c0-.638.517-1.155 1.156-1.155V5.6A2.656 2.656 0 0 0 1.25 8.256h1.5Zm1.156-1.155h9.23V5.6h-9.23v1.5Zm15.996 10.685V12.68h-1.5v5.106h1.5ZM1.812 10.88l6.12 3.162.69-1.332-6.12-3.162-.69 1.332Zm11.75 2.847 3.032-2.024-.833-1.248-3.031 2.024.833 1.248Zm2.72-6.62h2.86v-1.5h-2.86v1.5Zm2.86 0H22v-1.5h-2.859v1.5Zm.75 2.11v-2.86h-1.5v2.86h1.5Zm0-2.86V3.5h-1.5v2.859h1.5Zm-11.96 7.685c.87.45 1.453.76 2.078.86l.235-1.482c-.33-.052-.662-.214-1.624-.71l-.688 1.332Zm4.798-1.563c-.9.601-1.213.8-1.535.888l.4 1.446c.61-.168 1.154-.543 1.968-1.086l-.833-1.248Zm-2.72 2.423a3.75 3.75 0 0 0 1.584-.09l-.399-1.445a2.257 2.257 0 0 1-.95.053l-.235 1.482Z"></path></svg></div>Subscibe for updates</div><div class="shrink-0"></div></button></div></div><article class="flex flex-col items-start gap-10 pb-10"><div class="max-w-screen-lg px-5 mx-auto prose md:prose-xl dark:prose-invert prose-h1:text-center"><h1 class="">Resolving High Disk Space Utilization in MongoDB</h1></div><div class="flex-row items-center justify-center hidden w-full gap-5 md:flex text-slate-700 dark:text-neutral-300"><div class="flex items-center gap-2"><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1665498938042/VRcVGyEcb.jpeg" class="w-10 h-10 rounded-full" alt="Vamsi Rao"/><div class="text-base font-bold text-slate-600 dark:text-neutral-300">Vamsi Rao</div></div><time dateTime="2023-06-22T06:30:39.500Z">June 21, 2023</time></div><div class="w-full px-5 sm:mx-0"><div class="sm:mx-0"><div class="relative pt-[56.25%]"><img alt="Cover Image for Resolving High Disk Space Utilization in MongoDB" loading="lazy" decoding="async" data-nimg="fill" class="border dark:border-neutral-600 rounded-xl w-full" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;object-fit:cover;color:transparent" sizes="100vw" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=640&amp;q=75 640w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=750&amp;q=75 750w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=828&amp;q=75 828w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=1080&amp;q=75 1080w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=1200&amp;q=75 1200w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=1920&amp;q=75 1920w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=2048&amp;q=75 2048w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=3840&amp;q=75 3840w" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1686811735224%2F86d7f207-efe4-481c-a5d1-766a621cf262.png&amp;w=3840&amp;q=75"/></div></div></div><div class="w-full px-5 mx-auto md:max-w-screen-md hashnode-content-style"><h3 id="heading-problem">Problem</h3>
<p>We have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.</p>
<p>According to docs, <a target="_blank" href="https://www.mongodb.com/docs/atlas/reference/alert-resolutions/disk-io-utilization/">Disk utilization % on Data Partition</a> occurs if the percentage of time during which requests are being issued to any partition that contains the MongoDB collection data meets or exceeds the threshold. The downside of the Disk Utilization being high is the DB cannot process other queries if it maxes out. This can lead to data loss or inconsistencies.</p>
<p>These are how the metrics looked like in one of the last alerts. From the graph, Disk Util % at 4:30UTC was around 99.86%.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1687156481558/9c36fb63-e013-43cb-81da-082359d0038a.png" alt="Max disk util % metric hitting maxing out at 100%" class="image--center mx-auto" /></p>
<p>Checking the profiler, we can notice a query that's running during the same time, and it takes around <code>17s</code> to resolve!</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1682316589744/77e990f3-5a5e-4285-9e72-def7d2465829.png" alt="a screenshot of a computer screen with a number of numbers on it" class="image--center mx-auto" /></p>
<p>This query is related to a <code>KinesisAnalyticsLogs</code> collection. The collection internally is used to record views of different posts. The mentioned query already makes use of an index and still takes that much time to resolve because of the sheer size of the collection.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1682312447723/bcf12123-229c-4247-bdf3-7bab64c757fa.png" alt="a screen shot of a web page with a number of items on it" class="image--center mx-auto" /></p>
<p>The total number of documents is close to 70 million, and the index size itself is close to a GB! That seems to be why it takes so long to resolve. Since this collection was recording analytical data, it was bound to reach this volume at some point. Along with that, from the profiler image, we can see that the query has yielded ~1300 times. According to <a target="_blank" href="https://www.mongodb.com/docs/manual/reference/database-profiler/#mongodb-data-system.profile.numYield">docs</a>, if a query yields a lot, then it is hitting a disk a lot. If we want that query to be faster, then the data needs to be in memory (index).</p>
<p>Upon digging further, this query is scheduled to run every 30mins to sync the views. So we can correlate high query times on the profiler and Disk IOPS peaking almost simultaneously.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1682317743687/01f4c579-2d4a-4fec-bec3-9c3fec72bc76.png" class="image--center mx-auto" /></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1682317793584/30aa75ff-0132-4b1f-9547-e217f158b5f1.png" class="image--center mx-auto" /></p>
<h3 id="heading-solutions">Solutions</h3>
<p>Based on the investigation, we came up with two solutions:</p>
<p><strong>Short Term Solution</strong></p>
<ul>
<li><p>Since the collection size is a problem, and we are not using older data, deleting records older than a month will reduce the collection size drastically, leading to a smaller index size and faster query resolution.</p>
</li>
<li><p>We can also add <code>TTL</code> to the records in <code>kinesisAnalyticsLogs</code> collection (<a target="_blank" href="https://hashnode.com/rix/share/sg4lYYf_M">https://hashnode.com/rix/share/sg4lYYf_M</a>). It'll automatically delete the records older than a month going forward. This will make the index smaller and lead to a shorter query time.</p>
</li>
</ul>
<p><strong>Long Term Solution</strong></p>
<p>Data like views/analytics should not be stored in the same place as the core business data. This collection will keep growing by the minute since it records the views. Some other DB should be used that's more appropriate for it.</p>
<h3 id="heading-implementation">Implementation</h3>
<p>We decided to go with the short-term solution for now and added the long-term solution as a future task. For starters, we added TTL indexes immediately. With this, all the future records that will be created will be automatically deleted after the expiry time. This index can only be set on a field type <code>date</code></p>
<pre><code class="lang-javascript">kinesisAnalyticsLogSchema.index({ <span class="hljs-attr">dateAdded</span>: <span class="hljs-number">1</span> }, { <span class="hljs-attr">expireAfterSeconds</span>: <span class="hljs-number">2592000</span> });
</code></pre>
<p>To delete the past records, we ran a script that can delete all the records older than a month. Since we were deleting huge amounts of data within a short span, we encountered some problems while running the script. We had to keep a close eye on some of the metrics so that it didn't lead to a DB restart.</p>
<ul>
<li><p><code>CPU% spikes</code> A large number of record deletions were leading to CPU% usage over 95. We had to be careful and gave enough breathers in between to the DB.</p>
</li>
<li><p><code>Replication Oplog Window has gone below 1 hour</code> This was a new alert that we came across. Since our instance had one primary and two secondary DBs (replicas), the secondary DBs require enough time to replicate the writes from the primary. We had to be careful not to go below the recommended 1-hour window.</p>
</li>
</ul>
<p>After carefully running the script, this is how the overall collection looked like</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1686812657442/60eba67a-1754-4e8f-8c9f-d6ff3f2bc919.png" class="image--center mx-auto" /></p>
<p>This was almost 2-3 days of effort to run the script and observe how the DB was performing. We finally were seeing the difference. The query resolution was fast enough for a background aggregate task, and it was not creating the disk util alerts ðŸŽ‰</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1686817342070/33c9d255-4785-40b5-bdf1-5b5b9c4a02e9.png" class="image--center mx-auto" /></p>
<p>Overall improvements:</p>
<ul>
<li><p>Average query time went down to <strong>1.5s</strong> from <strong>15s</strong></p>
</li>
<li><p>The index size went down to <strong>~400MB</strong> from <strong>~1GB</strong></p>
</li>
<li><p>The collection size went down to <strong>~9GB</strong> from <strong>~40GB</strong></p>
</li>
</ul>
</div></article></div></main></div><footer class="px-5 py-20 border-t bg-slate-100 dark:bg-neutral-900 dark:border-neutral-800"><div class="container mx-auto grid grid-cols-1 gap-10 md:grid-cols-2"><div class="flex flex-col items-start col-span-1 gap-10 text-slate-500 dark:text-neutral-300"><h3 class="text-lg md:text-xl">The headless blog starter kit by Hashnode. Built with Next.js, TailwindCSS and Hashnode GraphQL APIs.</h3><div class="flex flex-row items-center gap-2 p-5 text-slate-600 bg-slate-200 rounded-xl"><p class="">Blog powered by</p><a href="#" class="flex flex-row items-center gap-1 font-semibold hover:underline"><svg class="w-5 h-5 stroke-current" fill="none" viewBox="0 0 24 24"><path stroke="currentColor" stroke-width="1.5" d="M7.314 4.97c1.64-1.64 2.461-2.46 3.407-2.767a4.143 4.143 0 0 1 2.56 0c.946.307 1.766 1.127 3.407 2.768l2.341 2.341c1.64 1.64 2.46 2.46 2.768 3.407.27.832.27 1.728 0 2.56-.307.946-1.127 1.766-2.768 3.407l-2.343 2.343c-1.64 1.64-2.461 2.46-3.407 2.768-.832.27-1.728.27-2.56 0-.946-.307-1.766-1.127-3.407-2.768l-2.341-2.341c-1.64-1.64-2.46-2.46-2.768-3.407a4.143 4.143 0 0 1 0-2.56C2.51 9.775 3.33 8.955 4.97 7.314l2.343-2.343Z"></path><path stroke="currentColor" stroke-width="1.5" d="M15.107 12a3.107 3.107 0 1 1-6.214 0 3.107 3.107 0 0 1 6.214 0Z"></path></svg>Hashnode</a></div></div><div class="flex flex-row items-center justify-start col-span-1 gap-2 md:justify-end text-slate-600 dark:text-neutral-300"><a class="hover:underline" href="#">Privacy</a><a class="hover:underline" href="#">Terms</a><p>Â© Company 2023</p></div></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"resolving-high-disk-space-utilization-in-mongodb","url":"https://engineering.hashnode.com/resolving-high-disk-space-utilization-in-mongodb","brief":"Problem\nWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\nAccording to docs, Disk utilization % on Data Partition occurs if the percentage of time during which requests are being issued to any partit...","title":"Resolving High Disk Space Utilization in MongoDB","publishedAt":"2023-06-22T06:30:39.500Z","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686811735224/86d7f207-efe4-481c-a5d1-766a621cf262.png"},"author":{"name":"Vamsi Rao","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1665498938042/VRcVGyEcb.jpeg"},"id":"6493ea8ff012983651c312d1","content":{"markdown":"### Problem\n\nWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\n\nAccording to docs, [Disk utilization % on Data Partition](https://www.mongodb.com/docs/atlas/reference/alert-resolutions/disk-io-utilization/) occurs if the percentage of time during which requests are being issued to any partition that contains the MongoDB collection data meets or exceeds the threshold. The downside of the Disk Utilization being high is the DB cannot process other queries if it maxes out. This can lead to data loss or inconsistencies.\n\nThese are how the metrics looked like in one of the last alerts. From the graph, Disk Util % at 4:30UTC was around 99.86%.\n\n![Max disk util % metric hitting maxing out at 100%](https://cdn.hashnode.com/res/hashnode/image/upload/v1687156481558/9c36fb63-e013-43cb-81da-082359d0038a.png align=\"center\")\n\nChecking the profiler, we can notice a query that's running during the same time, and it takes around `17s` to resolve!\n\n![a screenshot of a computer screen with a number of numbers on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1682316589744/77e990f3-5a5e-4285-9e72-def7d2465829.png align=\"center\")\n\nThis query is related to a `KinesisAnalyticsLogs` collection. The collection internally is used to record views of different posts. The mentioned query already makes use of an index and still takes that much time to resolve because of the sheer size of the collection.\n\n![a screen shot of a web page with a number of items on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1682312447723/bcf12123-229c-4247-bdf3-7bab64c757fa.png align=\"center\")\n\nThe total number of documents is close to 70 million, and the index size itself is close to a GB! That seems to be why it takes so long to resolve. Since this collection was recording analytical data, it was bound to reach this volume at some point. Along with that, from the profiler image, we can see that the query has yielded ~1300 times. According to [docs](https://www.mongodb.com/docs/manual/reference/database-profiler/#mongodb-data-system.profile.numYield), if a query yields a lot, then it is hitting a disk a lot. If we want that query to be faster, then the data needs to be in memory (index).\n\nUpon digging further, this query is scheduled to run every 30mins to sync the views. So we can correlate high query times on the profiler and Disk IOPS peaking almost simultaneously.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682317743687/01f4c579-2d4a-4fec-bec3-9c3fec72bc76.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682317793584/30aa75ff-0132-4b1f-9547-e217f158b5f1.png align=\"center\")\n\n### Solutions\n\nBased on the investigation, we came up with two solutions:\n\n**Short Term Solution**\n\n* Since the collection size is a problem, and we are not using older data, deleting records older than a month will reduce the collection size drastically, leading to a smaller index size and faster query resolution.\n    \n* We can also add `TTL` to the records in `kinesisAnalyticsLogs` collection ([https://hashnode.com/rix/share/sg4lYYf\\_M](https://hashnode.com/rix/share/sg4lYYf_M)). It'll automatically delete the records older than a month going forward. This will make the index smaller and lead to a shorter query time.\n    \n\n**Long Term Solution**\n\nData like views/analytics should not be stored in the same place as the core business data. This collection will keep growing by the minute since it records the views. Some other DB should be used that's more appropriate for it.\n\n### Implementation\n\nWe decided to go with the short-term solution for now and added the long-term solution as a future task. For starters, we added TTL indexes immediately. With this, all the future records that will be created will be automatically deleted after the expiry time. This index can only be set on a field type `date`\n\n```javascript\nkinesisAnalyticsLogSchema.index({ dateAdded: 1 }, { expireAfterSeconds: 2592000 });\n```\n\nTo delete the past records, we ran a script that can delete all the records older than a month. Since we were deleting huge amounts of data within a short span, we encountered some problems while running the script. We had to keep a close eye on some of the metrics so that it didn't lead to a DB restart.\n\n* `CPU% spikes` A large number of record deletions were leading to CPU% usage over 95. We had to be careful and gave enough breathers in between to the DB.\n    \n* `Replication Oplog Window has gone below 1 hour` This was a new alert that we came across. Since our instance had one primary and two secondary DBs (replicas), the secondary DBs require enough time to replicate the writes from the primary. We had to be careful not to go below the recommended 1-hour window.\n    \n\nAfter carefully running the script, this is how the overall collection looked like\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686812657442/60eba67a-1754-4e8f-8c9f-d6ff3f2bc919.png align=\"center\")\n\nThis was almost 2-3 days of effort to run the script and observe how the DB was performing. We finally were seeing the difference. The query resolution was fast enough for a background aggregate task, and it was not creating the disk util alerts ðŸŽ‰\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686817342070/33c9d255-4785-40b5-bdf1-5b5b9c4a02e9.png align=\"center\")\n\nOverall improvements:\n\n* Average query time went down to **1.5s** from **15s**\n    \n* The index size went down to **~400MB** from **~1GB**\n    \n* The collection size went down to **~9GB** from **~40GB**","html":"\u003ch3 id=\"heading-problem\"\u003eProblem\u003c/h3\u003e\n\u003cp\u003eWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\u003c/p\u003e\n\u003cp\u003eAccording to docs, \u003ca target=\"_blank\" href=\"https://www.mongodb.com/docs/atlas/reference/alert-resolutions/disk-io-utilization/\"\u003eDisk utilization % on Data Partition\u003c/a\u003e occurs if the percentage of time during which requests are being issued to any partition that contains the MongoDB collection data meets or exceeds the threshold. The downside of the Disk Utilization being high is the DB cannot process other queries if it maxes out. This can lead to data loss or inconsistencies.\u003c/p\u003e\n\u003cp\u003eThese are how the metrics looked like in one of the last alerts. From the graph, Disk Util % at 4:30UTC was around 99.86%.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1687156481558/9c36fb63-e013-43cb-81da-082359d0038a.png\" alt=\"Max disk util % metric hitting maxing out at 100%\" class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003cp\u003eChecking the profiler, we can notice a query that's running during the same time, and it takes around \u003ccode\u003e17s\u003c/code\u003e to resolve!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682316589744/77e990f3-5a5e-4285-9e72-def7d2465829.png\" alt=\"a screenshot of a computer screen with a number of numbers on it\" class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003cp\u003eThis query is related to a \u003ccode\u003eKinesisAnalyticsLogs\u003c/code\u003e collection. The collection internally is used to record views of different posts. The mentioned query already makes use of an index and still takes that much time to resolve because of the sheer size of the collection.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682312447723/bcf12123-229c-4247-bdf3-7bab64c757fa.png\" alt=\"a screen shot of a web page with a number of items on it\" class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003cp\u003eThe total number of documents is close to 70 million, and the index size itself is close to a GB! That seems to be why it takes so long to resolve. Since this collection was recording analytical data, it was bound to reach this volume at some point. Along with that, from the profiler image, we can see that the query has yielded ~1300 times. According to \u003ca target=\"_blank\" href=\"https://www.mongodb.com/docs/manual/reference/database-profiler/#mongodb-data-system.profile.numYield\"\u003edocs\u003c/a\u003e, if a query yields a lot, then it is hitting a disk a lot. If we want that query to be faster, then the data needs to be in memory (index).\u003c/p\u003e\n\u003cp\u003eUpon digging further, this query is scheduled to run every 30mins to sync the views. So we can correlate high query times on the profiler and Disk IOPS peaking almost simultaneously.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682317743687/01f4c579-2d4a-4fec-bec3-9c3fec72bc76.png\" alt class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682317793584/30aa75ff-0132-4b1f-9547-e217f158b5f1.png\" alt class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003ch3 id=\"heading-solutions\"\u003eSolutions\u003c/h3\u003e\n\u003cp\u003eBased on the investigation, we came up with two solutions:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eShort Term Solution\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eSince the collection size is a problem, and we are not using older data, deleting records older than a month will reduce the collection size drastically, leading to a smaller index size and faster query resolution.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWe can also add \u003ccode\u003eTTL\u003c/code\u003e to the records in \u003ccode\u003ekinesisAnalyticsLogs\u003c/code\u003e collection (\u003ca target=\"_blank\" href=\"https://hashnode.com/rix/share/sg4lYYf_M\"\u003ehttps://hashnode.com/rix/share/sg4lYYf_M\u003c/a\u003e). It'll automatically delete the records older than a month going forward. This will make the index smaller and lead to a shorter query time.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLong Term Solution\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eData like views/analytics should not be stored in the same place as the core business data. This collection will keep growing by the minute since it records the views. Some other DB should be used that's more appropriate for it.\u003c/p\u003e\n\u003ch3 id=\"heading-implementation\"\u003eImplementation\u003c/h3\u003e\n\u003cp\u003eWe decided to go with the short-term solution for now and added the long-term solution as a future task. For starters, we added TTL indexes immediately. With this, all the future records that will be created will be automatically deleted after the expiry time. This index can only be set on a field type \u003ccode\u003edate\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-javascript\"\u003ekinesisAnalyticsLogSchema.index({ \u003cspan class=\"hljs-attr\"\u003edateAdded\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e }, { \u003cspan class=\"hljs-attr\"\u003eexpireAfterSeconds\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e2592000\u003c/span\u003e });\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo delete the past records, we ran a script that can delete all the records older than a month. Since we were deleting huge amounts of data within a short span, we encountered some problems while running the script. We had to keep a close eye on some of the metrics so that it didn't lead to a DB restart.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003eCPU% spikes\u003c/code\u003e A large number of record deletions were leading to CPU% usage over 95. We had to be careful and gave enough breathers in between to the DB.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ccode\u003eReplication Oplog Window has gone below 1 hour\u003c/code\u003e This was a new alert that we came across. Since our instance had one primary and two secondary DBs (replicas), the secondary DBs require enough time to replicate the writes from the primary. We had to be careful not to go below the recommended 1-hour window.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAfter carefully running the script, this is how the overall collection looked like\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1686812657442/60eba67a-1754-4e8f-8c9f-d6ff3f2bc919.png\" alt class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003cp\u003eThis was almost 2-3 days of effort to run the script and observe how the DB was performing. We finally were seeing the difference. The query resolution was fast enough for a background aggregate task, and it was not creating the disk util alerts ðŸŽ‰\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1686817342070/33c9d255-4785-40b5-bdf1-5b5b9c4a02e9.png\" alt class=\"image--center mx-auto\" /\u003e\u003c/p\u003e\n\u003cp\u003eOverall improvements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eAverage query time went down to \u003cstrong\u003e1.5s\u003c/strong\u003e from \u003cstrong\u003e15s\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe index size went down to \u003cstrong\u003e~400MB\u003c/strong\u003e from \u003cstrong\u003e~1GB\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eThe collection size went down to \u003cstrong\u003e~9GB\u003c/strong\u003e from \u003cstrong\u003e~40GB\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"},"ogMetaData":{"image":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686811773227/c8f20c9f-790e-4501-b002-7b17a90e14fa.png"}}},"__N_SSG":true},"page":"/[slug]","query":{"slug":"resolving-high-disk-space-utilization-in-mongodb"},"buildId":"0y8ML8Ut1GKcfVwKpwMCT","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>